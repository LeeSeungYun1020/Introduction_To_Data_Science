{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "이승윤 - HW 11-1 Spam Filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungYun1020/Introduction_To_Data_Science/blob/master/classroom/%EC%9D%B4%EC%8A%B9%EC%9C%A4_HW_11_1_Spam_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zik1Z3gHtjYW"
      },
      "source": [
        "# HW 11-1. Simple Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYBPhcJuuAu"
      },
      "source": [
        "## T1. Load a dataset\n",
        "\n",
        "The following code loads a dataset consisting of text messages and spam-ham labels.\n",
        "\n",
        "You can write your own code below the **\"TODOs\"** to answer the questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UUgT0MW8Rmj"
      },
      "source": [
        "### Questions:\n",
        "* Number of spam messges? [*747*]\n",
        "* Number of ham messages? [*4825*]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2napWf8tINR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f36239f-8789-4c09-e3ed-8c0ade2f1787"
      },
      "source": [
        "from typing import List, Tuple, Dict, Iterable, Set\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mlee-pnu/IDS/main/spam_dataset.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# TODOs\n",
        "df.head(10)\n",
        "numberOfSpam = len(df[df['Category']=='spam'])\n",
        "numberOfHam = len(df[df['Category']=='ham'])\n",
        "print(\"total:\", numberOfSpam + numberOfHam)\n",
        "print(\"spam:\", numberOfSpam)\n",
        "print(\"ham:\", numberOfHam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total: 5572\n",
            "spam: 747\n",
            "ham: 4825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wryED8lOBP"
      },
      "source": [
        "## T2. Spam filter for individual words\n",
        "\n",
        "We first defined a function ***tokenize()*** to convert a given text into a set of words. \n",
        "\n",
        "Using the function, we now try to count the frequency of each word in each class (spam and ham).\n",
        "\n",
        "Complete the following code and answer the following questions:\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9VmU28wPHcH"
      },
      "source": [
        "### Qeustions: \n",
        "*   Number of spam messages containing the word \"free\": [*170*]\n",
        "\n",
        "Let's assume we only care for the word \"free\" to determine if a message is a spam or not. Answer the following questions:\n",
        "\n",
        "*   P ( *ham* | *free* ) = [*0.2576419213973799*]\n",
        "*   Is this message spam? [*Yes*]\n",
        "\n",
        "*Note: Do not apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6rmDIBJnFNh"
      },
      "source": [
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         \n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  \n",
        "    return set(all_words)                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtcK6qXlrES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d782b9-f287-440d-ca17-1ba61388c247"
      },
      "source": [
        "tokens: Set[str] = set()\n",
        "token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "spam = df[df.Category == 'spam']\n",
        "ham = df[df.Category == 'ham']\n",
        "\n",
        "for msg in spam['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_spam_counts[token] += 1\n",
        "\n",
        "for msg in ham['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_ham_counts[token] += 1\n",
        "\n",
        "# TODOs\n",
        "word = \"free\"\n",
        "n_word_spam = token_spam_counts[word]   # frequency of the word in spam messages\n",
        "n_word_ham =  token_ham_counts[word]    # frequency of the word in ham messages\n",
        "print(f\"Number of '{word}' in spam msg\", n_word_spam)\n",
        "print(f\"Number of '{word}' in ham msg\", n_word_ham)\n",
        "\n",
        "p_spam = numberOfSpam / (numberOfSpam + numberOfHam)      # P(spam)\n",
        "p_ham = numberOfHam / (numberOfSpam + numberOfHam)        # P(ham)\n",
        "p_word_given_spam = n_word_spam / numberOfSpam  # P(word|spam)\n",
        "p_word_given_ham = n_word_ham / numberOfHam     # P(word|ham)\n",
        "print(\"P(spam) =\", p_spam)\n",
        "print(\"P(ham) =\", p_ham)\n",
        "print(f\"P('{word}'|spam) =\", p_word_given_spam)\n",
        "print(f\"P('{word}'|ham) =\", p_word_given_ham)\n",
        "\n",
        "p_word = (n_word_spam + n_word_ham) / len(df)\n",
        "print(f\"P({'word'}) =\", p_word)\n",
        "\n",
        "# P(spam|word)\n",
        "p_spam_given_word = p_spam * p_word_given_spam / p_word\n",
        "# P(ham|word)\n",
        "p_ham_given_word = p_ham * p_word_given_ham / p_word\n",
        "print(f\"P(spam|'{word}') =\", p_spam_given_word)\n",
        "print(f\"P(ham|'{word}') =\", p_ham_given_word)\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"Number of spam messages containing 'free':\", n_word_spam)\n",
        "print(f\"P(ham|'{word}') = {p_ham_given_word}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of 'free' in spam msg 170\n",
            "Number of 'free' in ham msg 59\n",
            "P(spam) = 0.13406317300789664\n",
            "P(ham) = 0.8659368269921034\n",
            "P('free'|spam) = 0.22757697456492637\n",
            "P('free'|ham) = 0.0122279792746114\n",
            "P(word) = 0.04109834888729361\n",
            "P(spam|'free') = 0.74235807860262\n",
            "P(ham|'free') = 0.2576419213973799\n",
            "----------\n",
            "Number of spam messages containing 'free': 170\n",
            "P(ham|'free') = 0.2576419213973799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUzCC6l6knfU"
      },
      "source": [
        "## T3. Spam filter that combines words: Naive Bayes\n",
        "\n",
        "You received a text message \"just do it\" from an unknown sender.\n",
        "\n",
        "Complete the function ***predict()*** that outputs the probability of the message being spam and the predicted label of the message. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_a7X9Mj4CIs"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   P ( *spam* | *text* ) = [*3.3152855896268886e-06*]\n",
        "*   Is this text message spam? [*No*]\n",
        "\n",
        "*Note: You do not need to apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDTL_uBGL86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14874b09-d17e-407e-a86f-835b55794eec"
      },
      "source": [
        "text = \"just do it\"\n",
        "#text = \"Call to your phone\"\n",
        "#text = \"Free mobile text now\"\n",
        "#text = \"I and you\"\n",
        "#text = \"call to me\"\n",
        "\n",
        "# TODOs\n",
        "def predict(text: str):\n",
        "  text_tokens = tokenize(text)\n",
        "  log_spam = 0.0\n",
        "  log_ham = 0.0\n",
        "\n",
        "  for token in tokens:\n",
        "    pt_spam = token_spam_counts[token] / numberOfSpam\n",
        "    pt_ham = token_ham_counts[token] / numberOfHam\n",
        "    if token in text_tokens:\n",
        "      log_spam += math.log(pt_spam)\n",
        "      log_ham += math.log(pt_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - pt_spam)\n",
        "      log_ham += math.log(1.0 - pt_ham)\n",
        "  p_spam = math.exp(log_spam)\n",
        "  p_ham = math.exp(log_ham)\n",
        "  label = \"ham\"\n",
        "  if p_spam > p_ham:\n",
        "    label = \"spam\"\n",
        "  prob = p_spam / (p_spam + p_ham)\n",
        "  return prob, label\n",
        "\n",
        "print(predict(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3.3152855896268886e-06, 'ham')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQ3M91eTG7I"
      },
      "source": [
        "## T4. Smoothing method\n",
        "\n",
        "You again received two text messages from unknown senders.\n",
        "\n",
        "Complete the function ***spamFilter()*** that classifies a given message. \n",
        "\n",
        "You may want to apply a smoothing method for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBahJB5Tlat"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   Is textA spam?: [*Yes*]\n",
        "*   Is textB spam?: [*No*]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ66DoXVTpR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6798d30b-c686-473d-ea8d-033cf601bd8e"
      },
      "source": [
        "textA = \"reward! download your free ticket from our website www.pnu.edu\"\n",
        "textB = \"call me and get your money back\"\n",
        "\n",
        "# TODOs\n",
        "def spamFilter(text: str):\n",
        "  k = 0.5\n",
        "  text_tokens = tokenize(text)\n",
        "  log_spam = 0.0\n",
        "  log_ham = 0.0\n",
        "\n",
        "  for token in tokens:\n",
        "    pt_spam = (token_spam_counts[token] + k) / (numberOfSpam + 2 * k)\n",
        "    pt_ham = (token_ham_counts[token] + k) / (numberOfHam + 2 * k)\n",
        "    if token in text_tokens:\n",
        "      log_spam += math.log(pt_spam)\n",
        "      log_ham += math.log(pt_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - pt_spam)\n",
        "      log_ham += math.log(1.0 - pt_ham)\n",
        "  p_spam = math.exp(log_spam)\n",
        "  p_ham = math.exp(log_ham)\n",
        "  label = \"ham\"\n",
        "  if p_spam > p_ham:\n",
        "    label = \"spam\"\n",
        "  # prob = p_spam / (p_spam + p_ham)\n",
        "  # print(prob)\n",
        "  return label\n",
        "\n",
        "print(spamFilter(textA))\n",
        "print(spamFilter(textB))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam\n",
            "ham\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGcP_WWt4EmS"
      },
      "source": [
        "from collections import Counter\n",
        "predictions = [(category, spamFilter(message)) for category, message in zip(df['Category'], df['Message'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLZnOYfv7lFz",
        "outputId": "8b16d414-7a5a-45b2-aaf6-972d8e66cd6b"
      },
      "source": [
        "confusion_matrix = Counter((category == 'spam', is_spam == 'spam') for category, is_spam in predictions)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({(False, False): 4819, (True, True): 722, (True, False): 25, (False, True): 6})\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}